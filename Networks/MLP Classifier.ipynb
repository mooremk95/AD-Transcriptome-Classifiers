{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from itertools import permutations \n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfier = MLPClassifier(hidden_layer_sizes=(7,), random_state = 13, solver=\"lbfgs\", verbose=True)\n",
    "data = pd.read_csv(\"../Data/95_percent_var_PCs.csv\")\n",
    "# Encode the 'AD' and 'ND' classificaitons as integers. 0 for ND, 1 for AD\n",
    "data.loc[:,\"target\"] = data.loc[:,\"target\"].apply(lambda c: int(c == \"AD\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing data\n",
    "train_data, test_data = train_test_split(data, train_size=0.8, random_state=23)\n",
    "# training lalbels and training data\n",
    "train_lbls = train_data[\"target\"]\n",
    "train_data = train_data.loc[:, \"PC1\":\"PC164\"]\n",
    "# testing labels and test data\n",
    "test_lbls = test_data[\"target\"]\n",
    "test_data = test_data.loc[:, \"PC1\":\"PC164\"]\n",
    "# Scale the data so that it's standardized (mean = 0, variance = 1)\n",
    "\n",
    "scaler = StandardScaler().fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparameters_2layer = {\\n                \"alpha\": list(10.0 ** -np.arange(1,7)),\\n                \"hidden_layer_sizes\": list(permutations([i for i in range(1,300,5)],2)) \\n                }\\n\\nparameters_3layer = {\\n                \"alpha\": list(10.0 ** -np.arange(1,7)),\\n                \"hidden_layer_sizes\": list(permutations([i for i in range(1,300,10)],3)) \\n                }\\n  \\nparameters_4layer = {\\n                \"alpha\": list(10.0 ** -np.arange(1,7)),\\n                \"hidden_layer_sizes\": list(permutations([i for i in range(10,300,25)],4)) \\n                }    \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_1layer_cfier = MLPClassifier(max_iter=300, solver=\"lbfgs\")\n",
    "mlp_2layer_cfier = MLPClassifier(max_iter=300, solver=\"lbfgs\")\n",
    "mlp_3layer_cfier = MLPClassifier(max_iter=300, solver=\"lbfgs\")\n",
    "\n",
    "alpha_range = 10.0 ** -np.arange(1,7) # As suggested by sklearn documentation \n",
    "# The parameters dictionary defines the search grid for the hyperparameters being optimized over\n",
    "# Each dictionary key is the name the pipeline stage, followed by that stage's parameter being optimized.  \n",
    "parameters_1layer = {\n",
    "                \"alpha\": list(10.0 ** -np.arange(1,7)),\n",
    "                \"hidden_layer_sizes\": list(permutations([i for i in range(1,300,1)],1)) \n",
    "                }\n",
    "\n",
    "parameters_2layer = {\n",
    "                \"alpha\": list(10.0 ** -np.arange(1,7)),\n",
    "                \"hidden_layer_sizes\": list(permutations([i for i in range(1,300,5)],2)) \n",
    "                }\n",
    "\n",
    "parameters_3layer = {\n",
    "                \"alpha\": list(10.0 ** -np.arange(1,7)),\n",
    "                \"hidden_layer_sizes\": list(permutations([i for i in range(1,300,10)],3)) \n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1794 candidates, totalling 8970 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1434 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1961 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3257 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4877 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5808 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6821 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7914 tasks      | elapsed:  7.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 1-layer architecture {'alpha': 1e-06, 'hidden_layer_sizes': (15,)}\n",
      "Accuracy score: 64.6774%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 8970 out of 8970 | elapsed:  7.8min finished\n",
      "/home/mike/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Move on to 2-layer architectures with 1-300 neurons, increments of 5 neurons\\ngrid_search_pipeline_2l = GridSearchCV(mlp_2layer_cfier, parameters_2layer, verbose=2, n_jobs=-1) \\n\\ngrid_search_pipeline_2l.fit(train_data, train_lbls)\\n\\nprint(f\"Best 2-layer architecture {grid_search_pipeline_2l.best_params_}\")\\nprint(f\"Accuracy score: {grid_search_pipeline_2l.best_score_:.4%}\")\\n\\n# Move on to 3-layer architectures with 1-300 neurons, increments of 10 neurons\\ngrid_search_pipeline_3l = GridSearchCV(mlp_3layer_cfier, parameters_3layer, verbose=2, n_jobs=-1) \\n\\ngrid_search_pipeline_3l.fit(train_data, train_lbls)\\n\\nprint(f\"Best 3-layer architecture {grid_search_pipeline_3l.best_params_}\")\\nprint(f\"Accuracy score: {grid_search_pipeline_3l.best_score_:.4%}\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the Grid Search. Each possible permunation of alpha and 1-layer architectures from 1-300 \n",
    "# n_jobs = 7 means use 6 of my 12 processors. \n",
    "\n",
    "# Start by searching 1-layer architectures with 1-300 neurons, increments of 1 neuron\n",
    "grid_search_pipeline = GridSearchCV(mlp_1layer_cfier, parameters_1layer, verbose=2, n_jobs=-1) \n",
    "\n",
    "grid_search_pipeline.fit(train_data, train_lbls)\n",
    "\n",
    "print(f\"Best 1-layer architecture {grid_search_pipeline.best_params_}\")\n",
    "print(f\"Accuracy score: {grid_search_pipeline.best_score_:.4%}\")\n",
    "\n",
    "\n",
    "# Move on to 2-layer architectures with 1-300 neurons, increments of 5 neurons\n",
    "grid_search_pipeline_2l = GridSearchCV(mlp_2layer_cfier, parameters_2layer, verbose=2, n_jobs=-1) \n",
    "\n",
    "grid_search_pipeline_2l.fit(train_data, train_lbls)\n",
    "\n",
    "print(f\"Best 2-layer architecture {grid_search_pipeline_2l.best_params_}\")\n",
    "print(f\"Accuracy score: {grid_search_pipeline_2l.best_score_:.4%}\")\n",
    "\n",
    "# Move on to 3-layer architectures with 1-300 neurons, increments of 10 neurons\n",
    "grid_search_pipeline_3l = GridSearchCV(mlp_3layer_cfier, parameters_3layer, verbose=2, n_jobs=-1) \n",
    "\n",
    "grid_search_pipeline_3l.fit(train_data, train_lbls)\n",
    "\n",
    "print(f\"Best 3-layer architecture {grid_search_pipeline_3l.best_params_}\")\n",
    "print(f\"Accuracy score: {grid_search_pipeline_3l.best_score_:.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_architectures.txt\", \"w\") as f:\n",
    "    results = f\"Best 1-hidden-layer score: {grid_search_pipeline.best_score_:.4%}\\n\\t{grid_search_pipeline.best_params_}\"\n",
    "    results += f\"\\n\\nBest 2-hidden-layer score: {grid_search_pipeline_2l.best_score_:.4%}\\n\\t{grid_search_pipeline_2l.best_params_}\"\n",
    "    results += f\"\\n\\nBest 3-hidden-layer score: {grid_search_pipeline_3l.best_score_:.4%}\\n\\t{grid_search_pipeline_3l.best_params_}\"\n",
    "    f.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 72.3992%\n",
      "Best estimator {'alpha': 0.1, 'hidden_layer_sizes': (90, 130, 60, 40)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best score: {grid_search_pipeline.best_score_:.4%}\")\n",
    "print(f\"Best estimator {grid_search_pipeline.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_pipeline.decis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian hyperparameter searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, Trials, fmin, tpe, space_eval, STATUS_OK, STATUS_FAIL\n",
    "from timeit import default_timer as timer\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with various search spaces\n",
    "I think it's wise to keep alpha as a uniform distribution. I'll start with normal distributions for the number now neurons in each layer and see where that takes things. \n",
    "\n",
    "1. Search space 1: normal distributions with large variance. \n",
    "    - layer 1: ~N(mu = 150 neurons, sigma = 65)\n",
    "    - layer 2: ~N(mu = 75 neurons, sigma = 32)\n",
    "    - alpha: uniform {0.1, 0.01,... 10^-7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 6.0, 'l1': 142.0, 'l2': 50.0, 'type': 'two_layer'}\n",
      "  0%|          | 0/750 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Stop this damn thing!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Stop this damn thing!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-30f12b27d944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Run the hyperparam search. We'll let the hyperopt lib suggest the tpe to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m best_architectue = fmin(fn=objective, space=search_space, algo=tpe.suggest, trials=trials, max_evals=750, \n\u001b[0;32m---> 36\u001b[0;31m                         rstate= np.random.RandomState(25))\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mtrials_save_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         )\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mtrials_save_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         )\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             )\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-30f12b27d944>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(architecture)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stop this damn thing!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Create a trials Hyperopt Object which stores data regarding the results of our hyperparameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Stop this damn thing!"
     ]
    }
   ],
   "source": [
    "# Define the search space. Dictionaries are in key:value where the value is a probabilistic space to \n",
    "# \"randomly\" chose from (I believe as TPE algorithm works, it changes these probabilistic distributions)\n",
    "search_space =  hp.choice(\"layers\", [\n",
    "    {\n",
    "        \"type\": \"two_layer\",\n",
    "        \"k\": hp.quniform(\"2_layer_k\",1,7,1), # k is selecting for bias alpha, where alpha = 10^-k [0.1, 0.01, ... 10^-7]\n",
    "        \"l1\": hp.qnormal(\"2_layer_l1\",150, 65, 2), # Discretized normal dist, mu =150, stdev=65\n",
    "        \"l2\": hp.qnormal(\"2_layer_l2\",75, 32, 2) #  mu =75, stdev=32\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"three_layer\",\n",
    "        \"k\": hp.uniform(\"3_layer_k\",1,7),\n",
    "        \"l1\": hp.qnormal(\"3layer_l1\",150, 65, 2), # Discretized normal dist, mu =150, stdev=65\n",
    "        \"l2\": hp.qnormal(\"3_layer_l2\",75, 32, 2), # mu =75, stdev=32\n",
    "        \"l3\": hp.qnormal(\"3_layer_l3\",40, 20, 2) # Discretized normal dist, mu =75, stdev=32\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the optimization function (Costly as it implies fitting a MLP classifier via CV)\n",
    "def objective(architecture):\n",
    "    \"\"\" The objective function to optimize is the 5-fold cross-validation fitting of a 2 hidden layer NN classifier.\n",
    "        The hyperoptimizer operates as a minimizer, so the returned loss will be the negative of the avg of \n",
    "        mean accuracy.\n",
    "    \"\"\"\n",
    "    # Determine classifer hyperparams from architecture dictionary given\n",
    "    if architecture[\"type\"] == \"two_layers\":\n",
    "        shape = (2, int(architecture[\"l1\"]), int(architecture[\"l2\"]))\n",
    "    else:\n",
    "        shape = (2, int(architecture[\"l1\"]), int(architecture[\"l2\"]), int(architecture[\"l3\"]))\n",
    "    k = architecture[\"k\"]\n",
    "    \n",
    "    # Make a new MLP Classifier\n",
    "    cfier = MLPClassifier(max_iter=300, hidden_layer_sizes=shape, solver= \"lbfgs\", alpha = pow(10, -))\n",
    "    \n",
    "    # Cross Validatae\n",
    "    \n",
    "    \n",
    "    print(architecture)\n",
    "    \n",
    "\n",
    "    \n",
    "# Create a trials Hyperopt Object which stores data regarding the results of our hyperparameter search\n",
    "trials = Trials()\n",
    "\n",
    "# Run the hyperparam search. We'll let the hyperopt lib suggest the tpe to use\n",
    "best_architectue = fmin(fn=objective, space=search_space, algo=tpe.suggest, trials=trials, max_evals=750, \n",
    "                        rstate= np.random.RandomState(25))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
